import os
import argparse
import torch
import json

from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoModelForSeq2SeqLM,  # é€‚é…Seq2Seqç”Ÿæˆä»»åŠ¡çš„AutoModelå¤´
    AutoTokenizer,
    get_linear_schedule_with_warmup,
    set_seed
)
from tqdm import tqdm  # è¿›åº¦æ¡ï¼ˆéœ€å®‰è£…ï¼špip install tqdmï¼‰
import warnings
warnings.filterwarnings("ignore")  # å¿½ç•¥æ— å…³è­¦å‘Š

# -------------------------- åŸºç¡€é…ç½®ï¼ˆä¸process.pyå¯¹é½ï¼‰ --------------------------
# å¤„ç†åæ•°æ®ç›®å½•ï¼ˆprocess.pyçš„è¾“å‡ºç›®å½•ï¼‰
PROCESSED_DATA_DIR = "D:\\python_project\\test\\ai_medical\\data\\spell_check\\processed"
# æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆè®­ç»ƒå®Œæˆåä¿å­˜æ¨¡å‹ï¼‰
MODEL_SAVE_DIR = "D:\\python_project\\test\\ai_medical\\models\\bart_correction"
# é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸­æ–‡BARTåŸºç¡€æ¨¡å‹ï¼Œé€‚é…ä¸­æ–‡çº é”™ä»»åŠ¡ï¼‰
PRETRAINED_MODEL = "D:\\python_project\\test\\ai_medical\\pretrained\\bart-base-chinese"
# è®¾å¤‡é…ç½®ï¼ˆä¼˜å…ˆGPUï¼Œæ— GPUåˆ™ç”¨CPUï¼‰
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# æ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆæ ¹æ®ä½ çš„å¥å­é•¿åº¦è°ƒæ•´ï¼ŒBART-baseå»ºè®®â‰¤128ï¼‰
MAX_LENGTH = 128
# éšæœºç§å­ï¼ˆä¿è¯è®­ç»ƒå¯å¤ç°ï¼‰
SEED = 42
set_seed(SEED)  # å›ºå®šæ‰€æœ‰éšæœºç§å­


# -------------------------- 1. æ•°æ®é›†ç±»ï¼ˆé€‚é…process.pyè¾“å‡ºçš„JSONæ ¼å¼ï¼‰ --------------------------
class CorrectionDataset(Dataset):
    """
    ä¸­æ–‡æ–‡æœ¬çº é”™æ•°æ®é›†ï¼šè¯»å–process.pyç”Ÿæˆçš„train.json/val.json/test.json
    æ¯æ¡æ ·æœ¬æ ¼å¼ï¼š{"input": é”™è¯¯å¥, "target": æ­£ç¡®å¥, "source_file": "...", "line_number": ...}
    """
    def __init__(self, data_path: str, tokenizer: AutoTokenizer):
        self.tokenizer = tokenizer  # BARTçš„Tokenizerï¼ˆç”¨äºæ–‡æœ¬ç¼–ç ï¼‰
        # è¯»å–JSONæ•°æ®ï¼ˆè°ƒç”¨process.pyç”Ÿæˆçš„æ–‡ä»¶ï¼‰
        with open(data_path, "r", encoding="utf-8") as f:
            self.data = json.load(f)
        # è¿‡æ»¤æ— æ•ˆæ ·æœ¬ï¼ˆåŒé‡ä¿é™©ï¼Œé¿å…process.pyæ¼ç­›çš„ç©ºæ ·æœ¬ï¼‰
        self.data = [
            sample for sample in self.data
            if isinstance(sample.get("input"), str) and isinstance(sample.get("target"), str)
            and sample["input"].strip() and sample["target"].strip()
        ]
        print(f"âœ… åŠ è½½æ•°æ®é›†ï¼š{os.path.basename(data_path)} | æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(self.data)}")

    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†æ€»æ ·æœ¬æ•°"""
        return len(self.data)

    def __getitem__(self, idx: int) -> dict:
        """
        æŒ‰ç´¢å¼•è·å–å•æ¡æ ·æœ¬ï¼Œè¿”å›ç¼–ç åçš„tensor
        :param idx: æ ·æœ¬ç´¢å¼•
        :return: åŒ…å«input_idsã€attention_maskã€labelsçš„å­—å…¸
        """
        sample = self.data[idx]
        input_text = sample["input"]    # æ¨¡å‹è¾“å…¥ï¼šé”™è¯¯å¥
        target_text = sample["target"]  # æ¨¡å‹è¾“å‡ºï¼šæ­£ç¡®å¥

        # 1. ç¼–ç è¾“å…¥æ–‡æœ¬ï¼ˆé”™è¯¯å¥ï¼‰ï¼šæ·»åŠ BARTçš„ç‰¹æ®Štokenï¼ˆ<s>å¼€å¤´ï¼Œ</s>ç»“å°¾ï¼‰
        input_encoding = self.tokenizer(
            input_text,
            max_length=MAX_LENGTH,
            padding="max_length",  # ä¸è¶³MAX_LENGTHè¡¥0
            truncation=True,       # è¶…è¿‡MAX_LENGTHæˆªæ–­
            return_tensors="pt"    # è¿”å›PyTorch tensor
        )

        # 2. ç¼–ç ç›®æ ‡æ–‡æœ¬ï¼ˆæ­£ç¡®å¥ï¼‰ï¼šBARTçš„labelsæ— éœ€attention_maskï¼Œä»…éœ€input_ids
        target_encoding = self.tokenizer(
            target_text,
            max_length=MAX_LENGTH,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # æŒ¤å‹ç»´åº¦ï¼ˆå»é™¤batchç»´åº¦ï¼ŒDataLoaderä¼šè‡ªåŠ¨æ·»åŠ batchï¼‰
        return {
            "input_ids": input_encoding["input_ids"].squeeze(0),  # (MAX_LENGTH,)
            "attention_mask": input_encoding["attention_mask"].squeeze(0),  # (MAX_LENGTH,)
            "labels": target_encoding["input_ids"].squeeze(0)  # (MAX_LENGTH,)ï¼Œç”¨äºè®¡ç®—æŸå¤±
        }


# -------------------------- 2. æ•°æ®åŠ è½½å‡½æ•°ï¼ˆåˆ›å»ºDataLoaderï¼Œæ”¯æŒæ‰¹é‡è®­ç»ƒï¼‰ --------------------------
def create_dataloader(
    data_path: str,
    tokenizer: AutoTokenizer,
    batch_size: int,
    shuffle: bool = True
) -> DataLoader:
    """
    åˆ›å»ºDataLoaderï¼ˆæ‰¹é‡åŠ è½½æ•°æ®ï¼Œæ”¯æŒå¤šçº¿ç¨‹åŠ é€Ÿï¼‰
    :param data_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚train.jsonï¼‰
    :param tokenizer: AutoTokenizerå®ä¾‹
    :param batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼Œ16GBæ˜¾å­˜å»ºè®®8-16ï¼‰
    :param shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®ï¼ˆè®­ç»ƒé›†Trueï¼ŒéªŒè¯/æµ‹è¯•é›†Falseï¼‰
    :return: DataLoaderå®ä¾‹
    """
    dataset = CorrectionDataset(data_path, tokenizer)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=2,  # å¤šçº¿ç¨‹è¯»å–ï¼ˆCPUæ ¸å¿ƒæ•°â‰¥2æ—¶å»ºè®®è®¾2ï¼Œé¿å…å¡é¡¿ï¼‰
        pin_memory=True,  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“ï¼ˆæœ‰GPUæ—¶å¯ç”¨ï¼‰
        drop_last=False  # ä¸ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸è¶³batchçš„æ ·æœ¬
    )
    return dataloader


# -------------------------- 3. è®­ç»ƒæ ¸å¿ƒå‡½æ•°ï¼ˆå•è½®epochè®­ç»ƒï¼‰ --------------------------
def train_one_epoch(
    model: AutoModelForSeq2SeqLM,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler.LambdaLR,
    epoch: int,
    args
) -> float:
    """
    è®­ç»ƒä¸€ä¸ªepochï¼ˆéå†ä¸€æ¬¡è®­ç»ƒé›†ï¼‰
    :param model: è®­ç»ƒçš„æ¨¡å‹
    :param dataloader: è®­ç»ƒé›†DataLoader
    :param optimizer: ä¼˜åŒ–å™¨ï¼ˆå¦‚AdamWï¼‰
    :param scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ§åˆ¶å­¦ä¹ ç‡è¡°å‡ï¼‰
    :param epoch: å½“å‰epochç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰
    :param args: å‘½ä»¤è¡Œå‚æ•°ï¼ˆå«epochsã€batch_sizeç­‰ï¼‰
    :return: è¯¥epochçš„å¹³å‡è®­ç»ƒæŸå¤±
    """
    model.train()  # åˆ‡æ¢æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨Dropoutç­‰ï¼‰
    total_loss = 0.0  # ç´¯è®¡æ€»æŸå¤±
    # è¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºå½“å‰epochã€æ‰¹æ¬¡ã€æŸå¤±ï¼‰
    progress_bar = tqdm(
        dataloader,
        desc=f"ğŸ“Œ Train Epoch {epoch+1}/{args.epochs}",
        unit="batch",
        colour="green"
    )

    for batch in progress_bar:
        # 1. å°†batchæ•°æ®ç§»åŠ¨åˆ°GPU/CPU
        input_ids = batch["input_ids"].to(DEVICE, non_blocking=True)
        attention_mask = batch["attention_mask"].to(DEVICE, non_blocking=True)
        labels = batch["labels"].to(DEVICE, non_blocking=True)

        # 2. æ¸…é›¶æ¢¯åº¦ï¼ˆé¿å…æ¢¯åº¦ç´¯ç§¯ï¼‰
        optimizer.zero_grad()

        # 3. å‰å‘ä¼ æ’­ï¼šæ¨¡å‹è¾“å‡ºlogitså’ŒæŸå¤±ï¼ˆAutoModelForSeq2SeqLMè‡ªåŠ¨è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼‰
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        loss = outputs.loss  # æ¨¡å‹å†…ç½®çš„æŸå¤±ï¼ˆåŸºäºlabelsè®¡ç®—ï¼‰

        # 4. åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        loss.backward()

        # 5. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ŒBARTæ¨¡å‹å¸¸ç”¨ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # 6. ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°
        optimizer.step()

        # 7. å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # 8. ç´¯è®¡æŸå¤±å¹¶æ›´æ–°è¿›åº¦æ¡
        batch_loss = loss.item()
        total_loss += batch_loss * input_ids.size(0)  # æŒ‰æ ·æœ¬æ•°åŠ æƒç´¯è®¡
        progress_bar.set_postfix({"batch_loss": f"{batch_loss:.4f}"})  # å®æ—¶æ˜¾ç¤ºæ‰¹æ¬¡æŸå¤±

    # è®¡ç®—è¯¥epochçš„å¹³å‡æŸå¤±ï¼ˆæ€»æŸå¤±/æ€»æ ·æœ¬æ•°ï¼‰
    avg_train_loss = total_loss / len(dataloader.dataset)
    print(f"âœ… Train Epoch {epoch+1} | Avg Loss: {avg_train_loss:.4f}\n")
    return avg_train_loss


# -------------------------- 4. éªŒè¯æ ¸å¿ƒå‡½æ•°ï¼ˆå•è½®epochéªŒè¯ï¼Œæ— æ¢¯åº¦æ›´æ–°ï¼‰ --------------------------
def val_one_epoch(
    model: AutoModelForSeq2SeqLM,
    dataloader: DataLoader,
    epoch: int,
    args
) -> float:
    """
    éªŒè¯ä¸€ä¸ªepochï¼ˆéå†ä¸€æ¬¡éªŒè¯é›†ï¼Œä¸æ›´æ–°æ¨¡å‹å‚æ•°ï¼‰
    :param model: è®­ç»ƒçš„æ¨¡å‹
    :param dataloader: éªŒè¯é›†DataLoader
    :param epoch: å½“å‰epochç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰
    :param args: å‘½ä»¤è¡Œå‚æ•°
    :return: è¯¥epochçš„å¹³å‡éªŒè¯æŸå¤±
    """
    model.eval()  # åˆ‡æ¢æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropoutç­‰ï¼‰
    total_loss = 0.0
    progress_bar = tqdm(
        dataloader,
        desc=f"ğŸ“Œ Val Epoch {epoch+1}/{args.epochs}",
        unit="batch",
        colour="blue"
    )

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆåŠ é€ŸéªŒè¯ï¼ŒèŠ‚çœå†…å­˜ï¼‰
    with torch.no_grad():
        for batch in progress_bar:
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch["input_ids"].to(DEVICE, non_blocking=True)
            attention_mask = batch["attention_mask"].to(DEVICE, non_blocking=True)
            labels = batch["labels"].to(DEVICE, non_blocking=True)

            # å‰å‘ä¼ æ’­ï¼ˆä»…è®¡ç®—æŸå¤±ï¼Œä¸åå‘ä¼ æ’­ï¼‰
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            batch_loss = loss.item()
            total_loss += batch_loss * input_ids.size(0)

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({"batch_loss": f"{batch_loss:.4f}"})

    # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
    avg_val_loss = total_loss / len(dataloader.dataset)
    print(f"âœ… Val Epoch {epoch+1} | Avg Loss: {avg_val_loss:.4f}\n")
    return avg_val_loss


# -------------------------- 5. æ¨¡å‹ä¿å­˜å‡½æ•°ï¼ˆä¿å­˜æœ€ä¼˜æ¨¡å‹å’ŒTokenizerï¼‰ --------------------------
def save_best_model(
    model: AutoModelForSeq2SeqLM,
    tokenizer: AutoTokenizer,
    save_dir: str,
    epoch: int,
    val_loss: float
):
    """
    ä¿å­˜éªŒè¯æŸå¤±æœ€ä¼˜çš„æ¨¡å‹ï¼ˆé¿å…ä¿å­˜æ‰€æœ‰epochçš„æ¨¡å‹ï¼ŒèŠ‚çœç©ºé—´ï¼‰
    :param model: è®­ç»ƒçš„æ¨¡å‹
    :param tokenizer: AutoTokenizerå®ä¾‹ï¼ˆå¿…é¡»ä¸æ¨¡å‹é…å¥—ä¿å­˜ï¼‰
    :param save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
    :param epoch: å½“å‰epochç¼–å·
    :param val_loss: å½“å‰epochçš„éªŒè¯æŸå¤±
    """
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
    os.makedirs(save_dir, exist_ok=True)
    # ä¿å­˜è·¯å¾„ï¼ˆæ ‡æ³¨æœ€ä¼˜æ¨¡å‹çš„epochå’ŒæŸå¤±ï¼‰
    best_model_dir = os.path.join(save_dir, f"best_model_epoch{epoch+1}_valloss{val_loss:.4f}")
    # ä¿å­˜æ¨¡å‹å’ŒTokenizerï¼ˆHugging Faceæ ‡å‡†æ ¼å¼ï¼Œåç»­predict.pyå¯ç›´æ¥åŠ è½½ï¼‰
    model.save_pretrained(best_model_dir)
    tokenizer.save_pretrained(best_model_dir)
    print(f"ğŸ† ä¿å­˜æœ€ä¼˜æ¨¡å‹åˆ°ï¼š{best_model_dir}\n")


# -------------------------- 6. ä¸»å‡½æ•°ï¼ˆæ•´åˆå…¨æµç¨‹ï¼šåŠ è½½â†’è®­ç»ƒâ†’éªŒè¯â†’ä¿å­˜ï¼‰ --------------------------
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆæ”¯æŒåŠ¨æ€è°ƒæ•´è®­ç»ƒå‚æ•°ï¼Œæ— éœ€ä¿®æ”¹ä»£ç ï¼‰
    parser = argparse.ArgumentParser(description="è®­ç»ƒä¸­æ–‡æ–‡æœ¬çº é”™BARTæ¨¡å‹ï¼ˆé€‚é…process.pyè¾“å‡ºï¼‰")
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®10-20ï¼Œæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰")
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°ï¼ˆ16GBæ˜¾å­˜å»ºè®®8ï¼Œ24GBå»ºè®®16ï¼‰")
    parser.add_argument("--lr", type=float, default=3e-5, help="åˆå§‹å­¦ä¹ ç‡ï¼ˆBARTå¸¸ç”¨3e-5~5e-5ï¼‰")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹ï¼ˆå‰10%æ­¥æ•°ç¼“æ…¢å‡LRï¼‰")
    args = parser.parse_args()

    # æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯
    print("=" * 70)
    print("ğŸ“‹ è®­ç»ƒé…ç½®ä¿¡æ¯")
    print("=" * 70)
    print(f"é¢„è®­ç»ƒæ¨¡å‹ï¼š{PRETRAINED_MODEL}")
    print(f"è®¾å¤‡ï¼š{DEVICE}ï¼ˆGPUå‹å·ï¼š{torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'æ— '}ï¼‰")
    print(f"è®­ç»ƒè½®æ•°ï¼š{args.epochs} | æ‰¹æ¬¡å¤§å°ï¼š{args.batch_size} | åˆå§‹å­¦ä¹ ç‡ï¼š{args.lr}")
    print(f"è®­ç»ƒé›†è·¯å¾„ï¼š{os.path.join(PROCESSED_DATA_DIR, 'train.json')}")
    print(f"éªŒè¯é›†è·¯å¾„ï¼š{os.path.join(PROCESSED_DATA_DIR, 'val.json')}")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š{MODEL_SAVE_DIR}")
    print("=" * 70)

    try:
        # -------------------------- æ­¥éª¤1ï¼šåŠ è½½Tokenizerå’Œé¢„è®­ç»ƒæ¨¡å‹ --------------------------
        print("\nğŸ“¥ åŠ è½½Tokenizerå’Œé¢„è®­ç»ƒæ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)
        # åŠ è½½Seq2Seqç”Ÿæˆæ¨¡å‹ï¼ˆAutoModelForSeq2SeqLMè‡ªåŠ¨é€‚é…BARTçš„ç”Ÿæˆå¤´ï¼‰
        model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_MODEL)
        model = model.to(DEVICE)  # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        print("âœ… æ¨¡å‹å’ŒTokenizeråŠ è½½å®Œæˆï¼")

        # -------------------------- æ­¥éª¤2ï¼šåˆ›å»ºè®­ç»ƒé›†/éªŒè¯é›†DataLoader --------------------------
        print("\nğŸ“¥ åˆ›å»ºDataLoader...")
        train_loader = create_dataloader(
            data_path=os.path.join(PROCESSED_DATA_DIR, "train.json"),
            tokenizer=tokenizer,
            batch_size=args.batch_size,
            shuffle=True  # è®­ç»ƒé›†æ‰“ä¹±
        )
        val_loader = create_dataloader(
            data_path=os.path.join(PROCESSED_DATA_DIR, "val.json"),
            tokenizer=tokenizer,
            batch_size=args.batch_size,
            shuffle=False  # éªŒè¯é›†ä¸æ‰“ä¹±
        )

        # -------------------------- æ­¥éª¤3ï¼šåˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ --------------------------
        print("\nğŸ“¥ åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨...")
        # ä¼˜åŒ–å™¨ï¼šAdamWï¼ˆBARTæ¨¡å‹å¸¸ç”¨ï¼Œæ”¯æŒæƒé‡è¡°å‡ï¼‰
        optimizer = Adam(
            model.parameters(),
            lr=args.lr,
            weight_decay=0.01  # æƒé‡è¡°å‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        )
        # æ€»è®­ç»ƒæ­¥æ•° =  epochæ•° Ã— æ¯epochæ‰¹æ¬¡æ•°é‡
        total_training_steps = args.epochs * len(train_loader)
        # é¢„çƒ­æ­¥æ•° = æ€»æ­¥æ•° Ã— é¢„çƒ­æ¯”ä¾‹ï¼ˆå‰10%æ­¥æ•°å­¦ä¹ ç‡ä»0å‡åˆ°åˆå§‹LRï¼‰
        warmup_steps = int(total_training_steps * args.warmup_ratio)
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šçº¿æ€§é¢„çƒ­+çº¿æ€§è¡°å‡ï¼ˆBARTè®­ç»ƒå¸¸ç”¨ç­–ç•¥ï¼‰
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_training_steps
        )
        print(f"âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼æ€»è®­ç»ƒæ­¥æ•°ï¼š{total_training_steps} | é¢„çƒ­æ­¥æ•°ï¼š{warmup_steps}")

        # -------------------------- æ­¥éª¤4ï¼šå¼€å§‹è®­ç»ƒ+éªŒè¯å¾ªç¯ --------------------------
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        best_val_loss = float("inf")  # åˆå§‹åŒ–æœ€ä¼˜éªŒè¯æŸå¤±ï¼ˆæ— ç©·å¤§ï¼‰
        for epoch in range(args.epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            train_one_epoch(model, train_loader, optimizer, scheduler, epoch, args)
            # éªŒè¯ä¸€ä¸ªepoch
            val_loss = val_one_epoch(model, val_loader, epoch, args)
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆä»…å½“å½“å‰éªŒè¯æŸå¤±ä½äºå†å²æœ€ä¼˜æ—¶ä¿å­˜ï¼‰
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                save_best_model(model, tokenizer, MODEL_SAVE_DIR, epoch, best_val_loss)

        # -------------------------- æ­¥éª¤5ï¼šè®­ç»ƒå®Œæˆæ€»ç»“ --------------------------
        print("=" * 70)
        print("ğŸ‰ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
    finally:
        print("\nğŸ“ è®­ç»ƒæµç¨‹ç»“æŸï¼Œæ¸…ç†èµ„æº...")
        # æ¸…ç†GPUå†…å­˜ï¼ˆé¿å…æ˜¾å­˜å ç”¨ï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆï¼")
if __name__ == '__main__':
    main()